diff --git a/train_imagenet.py b/train_imagenet.py
index b48cb23..0c1fe5f 100644
--- a/train_imagenet.py
+++ b/train_imagenet.py
@@ -21,7 +21,7 @@ model_names = sorted(name for name in models.__dict__
     and callable(models.__dict__[name]))
 
 parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')
-parser.add_argument('data', metavar='DIR',
+parser.add_argument('--data', metavar='DIR', default="",
                     help='path to dataset')
 parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet',
                     help='model architecture: ' +
@@ -53,15 +53,38 @@ parser.add_argument("--seed", type=int, default=1234, metavar='BS', help='input
 parser.add_argument("--prefix", type=str, required=True, metavar='PFX', help='prefix for logging & checkpoint saving')
 parser.add_argument('--evaluate', dest='evaluate', action='store_true', help='evaluation only')
 parser.add_argument('--att-type', type=str, choices=['BAM', 'CBAM'], default=None)
+parser.add_argument('--cuda', action='store_true', default=False,
+                    help='use cuda')
+parser.add_argument('--dummy', action='store_true', default=False,
+                    help='use dummy data')
+parser.add_argument('--ipex', action='store_true', default=False,
+                    help='use ipex')
+parser.add_argument('--jit', action='store_true', default=False,
+                    help='use ipex')
+parser.add_argument('--precision', default="float32",
+                        help='precision, "float32" or "bfloat16", default is "float32"')
+parser.add_argument('--warmup', type=int, default=5,
+                    help='number of warmup')
+parser.add_argument('--max_iters', type=int, default=10,
+                    help='max number of iterations to run')
 best_prec1 = 0
 
+args = parser.parse_args()
+if args.ipex:
+    import intel_pytorch_extension as ipex
+    print("Running with IPEX...")
+    if args.precision == "bfloat16":
+        # Automatically mix precision
+        ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16)
+        print('Running with bfloat16...')
+
 if not os.path.exists('./checkpoints'):
     os.mkdir('./checkpoints')
 
 def main():
     global args, best_prec1
     global viz, train_lot, test_lot
-    args = parser.parse_args()
+    # args = parser.parse_args()
     print ("args", args)
 
     torch.manual_seed(args.seed)
@@ -78,11 +101,19 @@ def main():
     optimizer = torch.optim.SGD(model.parameters(), args.lr,
                             momentum=args.momentum,
                             weight_decay=args.weight_decay)
-    model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu)))
-    #model = torch.nn.DataParallel(model).cuda()
-    model = model.cuda()
+
+    if args.ipex:
+        model = torch.nn.DataParallel(model)
+        model = model.to(ipex.DEVICE)
+        # if args.jit:
+        #     model = torch.jit.script(model)
+    elif args.cuda:
+        model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu)))
+        model = model.cuda()
+    else:
+        model = torch.nn.DataParallel(model)
     print ("model")
-    print (model)
+    # print (model)
 
     # get the number of model parameters
     print('Number of model parameters: {}'.format(
@@ -103,26 +134,30 @@ def main():
         else:
             print("=> no checkpoint found at '{}'".format(args.resume))
 
+    if args.cuda:
+        cudnn.benchmark = True
+
+    if not args.dummy:
+        # Data loading code
+        traindir = os.path.join(args.data, 'train')
+        valdir = os.path.join(args.data, 'val')
+        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
+                                     std=[0.229, 0.224, 0.225])
+
+        # import pdb
+        # pdb.set_trace()
+        val_loader = torch.utils.data.DataLoader(
+            datasets.ImageFolder(valdir, transforms.Compose([
+                    transforms.Scale(256),
+                    transforms.CenterCrop(224),
+                    transforms.ToTensor(),
+                    normalize,
+                    ])),
+                batch_size=args.batch_size, shuffle=False,
+               num_workers=args.workers, pin_memory=True)
+    else:
+        val_loader = None
 
-    cudnn.benchmark = True
-
-    # Data loading code
-    traindir = os.path.join(args.data, 'train')
-    valdir = os.path.join(args.data, 'val')
-    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
-                                 std=[0.229, 0.224, 0.225])
-
-    # import pdb
-    # pdb.set_trace()
-    val_loader = torch.utils.data.DataLoader(
-        datasets.ImageFolder(valdir, transforms.Compose([
-                transforms.Scale(256),
-                transforms.CenterCrop(224),
-                transforms.ToTensor(),
-                normalize,
-                ])),
-            batch_size=args.batch_size, shuffle=False,
-           num_workers=args.workers, pin_memory=True)
     if args.evaluate:
         validate(val_loader, model, criterion, 0)
         return
@@ -177,8 +212,9 @@ def train(train_loader, model, criterion, optimizer, epoch):
     for i, (input, target) in enumerate(train_loader):
         # measure data loading time
         data_time.update(time.time() - end)
-        
-        target = target.cuda(async=True)
+
+        # target = target.cuda(async=True)
+        target = target.cuda()
         input_var = torch.autograd.Variable(input)
         target_var = torch.autograd.Variable(target)
         
@@ -220,37 +256,76 @@ def validate(val_loader, model, criterion, epoch):
     # switch to evaluate mode
     model.eval()
 
-    end = time.time()
-    for i, (input, target) in enumerate(val_loader):
-        target = target.cuda(async=True)
+    if args.dummy:
+        input = torch.randn(args.batch_size, 3, 224, 224)
         input_var = torch.autograd.Variable(input, volatile=True)
-        target_var = torch.autograd.Variable(target, volatile=True)
-        
-        # compute output
-        output = model(input_var)
-        loss = criterion(output, target_var)
-        
-        # measure accuracy and record loss
-        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))
-        losses.update(loss.data[0], input.size(0))
-        top1.update(prec1[0], input.size(0))
-        top5.update(prec5[0], input.size(0))
-        
-        # measure elapsed time
-        batch_time.update(time.time() - end)
-        end = time.time()
-        
-        if i % args.print_freq == 0:
-            print('Test: [{0}/{1}]\t'
-                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
-                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
-                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
-                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(
-                   i, len(val_loader), batch_time=batch_time, loss=losses,
-                   top1=top1, top5=top5))
-    
-    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'
+        if args.ipex:
+            input_var = input_var.to(ipex.DEVICE)
+            if args.jit:
+                model = torch.jit.trace(model, input_var)
+        elif args.cuda:
+            input_var = input_var.cuda()
+        for i in range(args.max_iters + args.warmup):
+            # compute output
+            if i >= args.warmup:
+                end = time.time()
+            output = model(input_var)
+
+            # measure elapsed time
+            if i >= args.warmup:
+                batch_time.update(time.time() - end)
+
+            if i % args.print_freq == 0:
+                print('Test: [{0}/{1}]\t'
+                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'.format(
+                       i, args.max_iters + args.warmup, batch_time=batch_time))
+    else:
+        for i, (input, target) in enumerate(val_loader):
+            if i >= args.warmup:
+                end = time.time()
+            # target = target.cuda(async=True)
+            if args.ipex:
+                target = target.to(ipex.DEVICE)
+                input_var = torch.autograd.Variable(input.to(ipex.DEVICE), volatile=True)
+                target_var = torch.autograd.Variable(target, volatile=True)
+            elif args.cuda:
+                target = target.cuda()
+                input_var = torch.autograd.Variable(input, volatile=True)
+                target_var = torch.autograd.Variable(target, volatile=True)
+            else:
+                target = target.cuda()
+                input_var = torch.autograd.Variable(input, volatile=True)
+                target_var = torch.autograd.Variable(target, volatile=True)
+
+            # compute output
+            output = model(input_var)
+            loss = criterion(output, target_var)
+
+            # measure accuracy and record loss
+            prec1, prec5 = accuracy(output.data, target, topk=(1, 5))
+            losses.update(loss.data[0], input.size(0))
+            top1.update(prec1[0], input.size(0))
+            top5.update(prec5[0], input.size(0))
+
+            # measure elapsed time
+            if i >= args.warmup:
+                batch_time.update(time.time() - end)
+
+            if i % args.print_freq == 0:
+                print('Test: [{0}/{1}]\t'
+                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
+                      'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
+                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
+                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(
+                       i, len(val_loader), batch_time=batch_time, loss=losses,
+                       top1=top1, top5=top5))
+
+        print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'
             .format(top1=top1, top5=top5))
+    latency = batch_time.avg / args.batch_size * 1000
+    perf = args.batch_size / batch_time.avg
+    print('inference latency: %0.3f ms' % latency)
+    print('inference Throughput: %0.3f fps' % perf)
 
     return top1.avg
 
